{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resnet50_LSTM GroupNorm Baseline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1GhnaxGU7t6G",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "4d4bff6e-75c9-486e-fda9-553371a6c7ad"
      },
      "cell_type": "code",
      "source": [
        "# Upload our Kaggle API token\n",
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d09515a-1e89-4449-9e80-23c93f92a664\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2d09515a-1e89-4449-9e80-23c93f92a664\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"dskswu\",\"key\":\"15c065ffd8ebf14a3d3dea306863abed\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "aoVTRSY57vNe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "02383512-c479-40d5-cd4b-837e67a14a3b"
      },
      "cell_type": "code",
      "source": [
        "!ls -lha kaggle.json\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 62 Jan  8 05:34 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0ukXraMM8qRq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2557
        },
        "outputId": "14e4c96f-5cee-40b0-9bdc-bd629d5f08f0"
      },
      "cell_type": "code",
      "source": [
        "!pip install keras-self-attention\n",
        "!pip install iterative-stratification\n",
        "!pip install keras\n",
        "!pip install imgaug\n",
        "!pip install tensorflow\n",
        "!pip install scikit-learn\n",
        "!pip install kaggle\n",
        "!pip install ipywidgets\n",
        "!pip install os\n",
        "!pip install random\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install time\n",
        "!pip install tqdm\n",
        "!pip install imgaug"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading https://files.pythonhosted.org/packages/b7/8f/92d1889a9319e50a8edea6446c218ee324d776b152e39ac49d43bdcb9ec6/keras-self-attention-0.32.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (1.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-self-attention) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-self-attention) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-self-attention) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-self-attention) (1.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-self-attention) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-self-attention) (1.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-self-attention) (1.0.6)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Running setup.py bdist_wheel for keras-self-attention ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c0/0b/2f/9fafdff28a19fae5a7aca5704b205e247d4bfbe170cfb21aea\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.32.0\n",
            "Collecting iterative-stratification\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.14.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.20.2)\n",
            "Installing collected packages: iterative-stratification\n",
            "Successfully installed iterative-stratification-0.1.6\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (0.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.1.0)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.11.0)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.2)\n",
            "Requirement already satisfied: pillow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (4.0.0)\n",
            "Requirement already satisfied: matplotlib>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (1.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=2.1.0->scikit-image>=0.11.0->imgaug) (0.46)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2018.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2.3.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.12.0)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.6.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.6.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.32.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.6)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (40.6.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.20.2)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.14.6)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.1.1)\n",
            "Requirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.11.29)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.0.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Requirement already satisfied: Unidecode>=0.04.16 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.0.23)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (7.4.2)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (4.3.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.4.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (3.4.2)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (4.4.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets) (4.6.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.6.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (40.6.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.3.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (2.1.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (1.0.15)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.8.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.1->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.1->ipywidgets) (1.11.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from widgetsnbextension~=3.4.0->ipywidgets) (5.2.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets) (2.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.4.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets) (4.5.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.2.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (2.10)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.8.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (5.4.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (17.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets) (2.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: mistune>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (1.4.2)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.4.0->ipywidgets) (0.5.1)\n",
            "Collecting os\n",
            "\u001b[31m  Could not find a version that satisfies the requirement os (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for os\u001b[0m\n",
            "Collecting random\n",
            "\u001b[31m  Could not find a version that satisfies the requirement random (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for random\u001b[0m\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.22.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.14.6)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.7)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas) (1.11.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.14.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2018.7)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
            "Collecting time\n",
            "\u001b[31m  Could not find a version that satisfies the requirement time (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for time\u001b[0m\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.6/dist-packages (0.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.1.0)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug) (1.11.0)\n",
            "Requirement already satisfied: pillow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (4.0.0)\n",
            "Requirement already satisfied: matplotlib>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.1.2)\n",
            "Requirement already satisfied: networkx>=1.8 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug) (1.0.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=2.1.0->scikit-image>=0.11.0->imgaug) (0.46)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2018.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.3.1->scikit-image>=0.11.0->imgaug) (2.5.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.8->scikit-image>=0.11.0->imgaug) (4.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eEJBoKsF7vQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "b5e9b394-e68f-426e-dd85-a4cf54ede79a"
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c human-protein-atlas-image-classification"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.csv to /content\n",
            "\r  0% 0.00/446k [00:00<?, ?B/s]\n",
            "100% 446k/446k [00:00<00:00, 30.6MB/s]\n",
            "Downloading train.csv to /content\n",
            "  0% 0.00/1.22M [00:00<?, ?B/s]\n",
            "100% 1.22M/1.22M [00:00<00:00, 39.8MB/s]\n",
            "Downloading test.zip to /content\n",
            "100% 4.36G/4.37G [01:09<00:00, 69.6MB/s]\n",
            "100% 4.37G/4.37G [01:09<00:00, 67.7MB/s]\n",
            "Downloading train.zip to /content\n",
            "100% 13.1G/13.1G [03:15<00:00, 72.1MB/s]\n",
            "100% 13.1G/13.1G [03:15<00:00, 71.8MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yEKoxLfj7vUv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e7728460-6a6a-4540-ea41-46fafce823b3"
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('train.zip', 'r')\n",
        "zip_ref.extractall('train')\n",
        "zip_ref.close()\n",
        "\n",
        "zip_ref = zipfile.ZipFile('test.zip', 'r')\n",
        "zip_ref.extractall('test')\n",
        "zip_ref.close()\n",
        "\n",
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kaggle.json  sample_submission.csv  test.zip  train.csv\n",
            "sample_data  test\t\t    train     train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4QGAYr_p7vYG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://github.com/fchollet/deep-learning-models/releases/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "356b8926c84ce7a8f9836d578ccc1ea0eb565fcb",
        "id": "OkX5ySOy6350",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage.io\n",
        "from skimage.transform import resize\n",
        "from imgaug import augmenters as iaa\n",
        "from tqdm import tqdm\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from sklearn.utils import class_weight, shuffle\n",
        "\n",
        "from itertools import chain\n",
        "from collections import Counter\n",
        "import threading\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "SIZE = 299"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "30838535796928f005982ba954dc34a4162089d8",
        "id": "Rxoz463W6351",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load dataset info\n",
        "path_to_train = 'train/'\n",
        "data = pd.read_csv('train.csv')\n",
        "\n",
        "train_dataset_info = []\n",
        "for name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n",
        "    train_dataset_info.append({\n",
        "        'path':os.path.join(path_to_train, name),\n",
        "        'labels':np.array([int(label) for label in labels])})\n",
        "train_dataset_info = np.array(train_dataset_info)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de5b7a85ffd5f2f253281d5c2378392bb8ffae25",
        "id": "UFzmG_Cn6355",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class data_generator:\n",
        "    def __init__(self):\n",
        "        self.idx_array = None\n",
        "        self.idx = 0\n",
        "        self.dataset_len = 0\n",
        "        self.first = True\n",
        "        self.threadLock = threading.Lock()\n",
        "    \n",
        "    def create_train(dataset_info, batch_size, shape, augument=True):\n",
        "        assert shape[2] == 3\n",
        "        while True:\n",
        "            dataset_info = shuffle(dataset_info)\n",
        "            for start in range(0, len(dataset_info), batch_size):\n",
        "                end = min(start + batch_size, len(dataset_info))\n",
        "                batch_images = []\n",
        "                X_train_batch = dataset_info[start:end]\n",
        "                batch_labels = np.zeros((len(X_train_batch), 28))\n",
        "                for i in range(len(X_train_batch)):\n",
        "                    image = data_generator.load_image(\n",
        "                        X_train_batch[i]['path'], shape)   \n",
        "                    if augument:\n",
        "                        image = data_generator.augment(image)\n",
        "                    batch_images.append(image/255.)\n",
        "                    batch_labels[i][X_train_batch[i]['labels']] = 1\n",
        "                yield np.array(batch_images, np.float32), batch_labels\n",
        "    def load_image(path, shape):\n",
        "        image_red_ch = Image.open(path+'_red.png')\n",
        "        image_yellow_ch = Image.open(path+'_yellow.png')\n",
        "        image_green_ch = Image.open(path+'_green.png')\n",
        "        image_blue_ch = Image.open(path+'_blue.png')\n",
        "        image = np.stack((\n",
        "        np.array(image_red_ch), \n",
        "        np.array(image_green_ch), \n",
        "        np.array(image_blue_ch)), -1)\n",
        "        image = cv2.resize(image, (shape[0], shape[1]))\n",
        "        return image\n",
        "\n",
        "    def augment(image):\n",
        "        augment_img = iaa.Sequential([\n",
        "            iaa.OneOf([\n",
        "                iaa.Affine(rotate=0),\n",
        "                iaa.Affine(rotate=90),\n",
        "                iaa.Affine(rotate=180),\n",
        "                iaa.Affine(rotate=270),\n",
        "                iaa.Fliplr(0.5),\n",
        "                iaa.Flipud(0.5),\n",
        "            ])], random_order=True)\n",
        "\n",
        "        image_aug = augment_img.augment_image(image)\n",
        "        return image_aug\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fff5e09be918d1e2d96584ab9991637be81aee2f",
        "id": "T_GnWT8u6356",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GroupNormalization"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e680b7aa2b2bc66488a7ea653aca5adef380700",
        "id": "GC2k09dC6358",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e6d39f2-019e-4772-bfdf-b235ecbb0824"
      },
      "cell_type": "code",
      "source": [
        "from keras.engine import Layer, InputSpec\n",
        "from keras import initializers\n",
        "from keras import regularizers\n",
        "from keras import constraints\n",
        "from keras import backend as K\n",
        "\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "\n",
        "\n",
        "class GroupNormalization(Layer):\n",
        "    \"\"\"Group normalization layer\n",
        "    Group Normalization divides the channels into groups and computes within each group\n",
        "    the mean and variance for normalization. GN's computation is independent of batch sizes,\n",
        "    and its accuracy is stable in a wide range of batch sizes\n",
        "    # Arguments\n",
        "        groups: Integer, the number of groups for Group Normalization.\n",
        "        axis: Integer, the axis that should be normalized\n",
        "            (typically the features axis).\n",
        "            For instance, after a `Conv2D` layer with\n",
        "            `data_format=\"channels_first\"`,\n",
        "            set `axis=1` in `BatchNormalization`.\n",
        "        epsilon: Small float added to variance to avoid dividing by zero.\n",
        "        center: If True, add offset of `beta` to normalized tensor.\n",
        "            If False, `beta` is ignored.\n",
        "        scale: If True, multiply by `gamma`.\n",
        "            If False, `gamma` is not used.\n",
        "            When the next layer is linear (also e.g. `nn.relu`),\n",
        "            this can be disabled since the scaling\n",
        "            will be done by the next layer.\n",
        "        beta_initializer: Initializer for the beta weight.\n",
        "        gamma_initializer: Initializer for the gamma weight.\n",
        "        beta_regularizer: Optional regularizer for the beta weight.\n",
        "        gamma_regularizer: Optional regularizer for the gamma weight.\n",
        "        beta_constraint: Optional constraint for the beta weight.\n",
        "        gamma_constraint: Optional constraint for the gamma weight.\n",
        "    # Input shape\n",
        "        Arbitrary. Use the keyword argument `input_shape`\n",
        "        (tuple of integers, does not include the samples axis)\n",
        "        when using this layer as the first layer in a model.\n",
        "    # Output shape\n",
        "        Same shape as input.\n",
        "    # References\n",
        "        - [Group Normalization](https://arxiv.org/abs/1803.08494)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 groups=32,\n",
        "                 axis=-1,\n",
        "                 epsilon=1e-5,\n",
        "                 center=True,\n",
        "                 scale=True,\n",
        "                 beta_initializer='zeros',\n",
        "                 gamma_initializer='ones',\n",
        "                 beta_regularizer=None,\n",
        "                 gamma_regularizer=None,\n",
        "                 beta_constraint=None,\n",
        "                 gamma_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(GroupNormalization, self).__init__(**kwargs)\n",
        "        self.supports_masking = True\n",
        "        self.groups = groups\n",
        "        self.axis = axis\n",
        "        self.epsilon = epsilon\n",
        "        self.center = center\n",
        "        self.scale = scale\n",
        "        self.beta_initializer = initializers.get(beta_initializer)\n",
        "        self.gamma_initializer = initializers.get(gamma_initializer)\n",
        "        self.beta_regularizer = regularizers.get(beta_regularizer)\n",
        "        self.gamma_regularizer = regularizers.get(gamma_regularizer)\n",
        "        self.beta_constraint = constraints.get(beta_constraint)\n",
        "        self.gamma_constraint = constraints.get(gamma_constraint)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dim = input_shape[self.axis]\n",
        "\n",
        "        if dim is None:\n",
        "            raise ValueError('Axis ' + str(self.axis) + ' of '\n",
        "                             'input tensor should have a defined dimension '\n",
        "                             'but the layer received an input with shape ' +\n",
        "                             str(input_shape) + '.')\n",
        "\n",
        "        if dim < self.groups:\n",
        "            raise ValueError('Number of groups (' + str(self.groups) + ') cannot be '\n",
        "                             'more than the number of channels (' +\n",
        "                             str(dim) + ').')\n",
        "\n",
        "        if dim % self.groups != 0:\n",
        "            raise ValueError('Number of groups (' + str(self.groups) + ') must be a '\n",
        "                             'multiple of the number of channels (' +\n",
        "                             str(dim) + ').')\n",
        "\n",
        "        self.input_spec = InputSpec(ndim=len(input_shape),\n",
        "                                    axes={self.axis: dim})\n",
        "        shape = (dim,)\n",
        "\n",
        "        if self.scale:\n",
        "            self.gamma = self.add_weight(shape=shape,\n",
        "                                         name='gamma',\n",
        "                                         initializer=self.gamma_initializer,\n",
        "                                         regularizer=self.gamma_regularizer,\n",
        "                                         constraint=self.gamma_constraint)\n",
        "        else:\n",
        "            self.gamma = None\n",
        "        if self.center:\n",
        "            self.beta = self.add_weight(shape=shape,\n",
        "                                        name='beta',\n",
        "                                        initializer=self.beta_initializer,\n",
        "                                        regularizer=self.beta_regularizer,\n",
        "                                        constraint=self.beta_constraint)\n",
        "        else:\n",
        "            self.beta = None\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        input_shape = K.int_shape(inputs)\n",
        "        tensor_input_shape = K.shape(inputs)\n",
        "\n",
        "        # Prepare broadcasting shape.\n",
        "        reduction_axes = list(range(len(input_shape)))\n",
        "        del reduction_axes[self.axis]\n",
        "        broadcast_shape = [1] * len(input_shape)\n",
        "        broadcast_shape[self.axis] = input_shape[self.axis] // self.groups\n",
        "        broadcast_shape.insert(1, self.groups)\n",
        "\n",
        "        reshape_group_shape = K.shape(inputs)\n",
        "        group_axes = [reshape_group_shape[i] for i in range(len(input_shape))]\n",
        "        group_axes[self.axis] = input_shape[self.axis] // self.groups\n",
        "        group_axes.insert(1, self.groups)\n",
        "\n",
        "        # reshape inputs to new group shape\n",
        "        group_shape = [group_axes[0], self.groups] + group_axes[2:]\n",
        "        group_shape = K.stack(group_shape)\n",
        "        inputs = K.reshape(inputs, group_shape)\n",
        "\n",
        "        group_reduction_axes = list(range(len(group_axes)))\n",
        "        group_reduction_axes = group_reduction_axes[2:]\n",
        "\n",
        "        mean = K.mean(inputs, axis=group_reduction_axes, keepdims=True)\n",
        "        variance = K.var(inputs, axis=group_reduction_axes, keepdims=True)\n",
        "\n",
        "        inputs = (inputs - mean) / (K.sqrt(variance + self.epsilon))\n",
        "\n",
        "        # prepare broadcast shape\n",
        "        inputs = K.reshape(inputs, group_shape)\n",
        "        outputs = inputs\n",
        "\n",
        "        # In this case we must explicitly broadcast all parameters.\n",
        "        if self.scale:\n",
        "            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)\n",
        "            outputs = outputs * broadcast_gamma\n",
        "\n",
        "        if self.center:\n",
        "            broadcast_beta = K.reshape(self.beta, broadcast_shape)\n",
        "            outputs = outputs + broadcast_beta\n",
        "\n",
        "        outputs = K.reshape(outputs, tensor_input_shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'groups': self.groups,\n",
        "            'axis': self.axis,\n",
        "            'epsilon': self.epsilon,\n",
        "            'center': self.center,\n",
        "            'scale': self.scale,\n",
        "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
        "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
        "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
        "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
        "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
        "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
        "        }\n",
        "        base_config = super(GroupNormalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "\n",
        "get_custom_objects().update({'GroupNormalization': GroupNormalization})"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "26a1a2801eba9feca75174ab2a1cbd72d44bf64e",
        "id": "c-9EoQcD635_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Squeeze-and-Excitation ResNets\n",
        "References:\n",
        "    - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
        "    - []() # added when paper is published on Arxiv\n",
        "'''\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Activation\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import GlobalMaxPooling2D\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import add\n",
        "from keras.layers import multiply\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import conv_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.engine.topology import get_source_inputs\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "from keras import backend as K\n",
        "import importlib\n",
        "\n",
        "#from layers.GroupNormalization import GroupNormalization\n",
        "\n",
        "WEIGHTS_PATH = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n",
        "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "def _obtain_input_shape(input_shape,\n",
        "                        default_size,\n",
        "                        min_size,\n",
        "                        data_format,\n",
        "                        require_flatten,\n",
        "                        weights=None):\n",
        "    \"\"\"Internal utility to compute/validate a model's input shape.\n",
        "    # Arguments\n",
        "        input_shape: Either None (will return the default network input shape),\n",
        "            or a user-provided shape to be validated.\n",
        "        default_size: Default input width/height for the model.\n",
        "        min_size: Minimum input width/height accepted by the model.\n",
        "        data_format: Image data format to use.\n",
        "        require_flatten: Whether the model is expected to\n",
        "            be linked to a classifier via a Flatten layer.\n",
        "        weights: One of `None` (random initialization)\n",
        "            or 'imagenet' (pre-training on ImageNet).\n",
        "            If weights='imagenet' input channels must be equal to 3.\n",
        "    # Returns\n",
        "        An integer shape tuple (may include None entries).\n",
        "    # Raises\n",
        "        ValueError: In case of invalid argument values.\n",
        "    \"\"\"\n",
        "    if weights != 'imagenet' and input_shape and len(input_shape) == 3:\n",
        "        if data_format == 'channels_first':\n",
        "            if input_shape[0] not in {1, 3}:\n",
        "                warnings.warn(\n",
        "                    'This model usually expects 1 or 3 input channels. '\n",
        "                    'However, it was passed an input_shape with ' +\n",
        "                    str(input_shape[0]) + ' input channels.')\n",
        "            default_shape = (input_shape[0], default_size, default_size)\n",
        "        else:\n",
        "            if input_shape[-1] not in {1, 3}:\n",
        "                warnings.warn(\n",
        "                    'This model usually expects 1 or 3 input channels. '\n",
        "                    'However, it was passed an input_shape with ' +\n",
        "                    str(input_shape[-1]) + ' input channels.')\n",
        "            default_shape = (default_size, default_size, input_shape[-1])\n",
        "    else:\n",
        "        if data_format == 'channels_first':\n",
        "            default_shape = (3, default_size, default_size)\n",
        "        else:\n",
        "            default_shape = (default_size, default_size, 3)\n",
        "    if weights == 'imagenet' and require_flatten:\n",
        "        if input_shape is not None:\n",
        "            if input_shape != default_shape:\n",
        "                raise ValueError('When setting`include_top=True` '\n",
        "                                 'and loading `imagenet` weights, '\n",
        "                                 '`input_shape` should be ' +\n",
        "                                 str(default_shape) + '.')\n",
        "        return default_shape\n",
        "    if input_shape:\n",
        "        if data_format == 'channels_first':\n",
        "            if input_shape is not None:\n",
        "                if len(input_shape) != 3:\n",
        "                    raise ValueError(\n",
        "                        '`input_shape` must be a tuple of three integers.')\n",
        "                if input_shape[0] != 3 and weights == 'imagenet':\n",
        "                    raise ValueError('The input must have 3 channels; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "                if ((input_shape[1] is not None and input_shape[1] < min_size) or\n",
        "                   (input_shape[2] is not None and input_shape[2] < min_size)):\n",
        "                    raise ValueError('Input size must be at least ' +\n",
        "                                     str(min_size) + 'x' + str(min_size) +\n",
        "                                     '; got `input_shape=' +\n",
        "                                     str(input_shape) + '`')\n",
        "        else:\n",
        "            if input_shape is not None:\n",
        "                if len(input_shape) != 3:\n",
        "                    raise ValueError(\n",
        "                        '`input_shape` must be a tuple of three integers.')\n",
        "                if input_shape[-1] != 3 and weights == 'imagenet':\n",
        "                    raise ValueError('The input must have 3 channels; got '\n",
        "                                     '`input_shape=' + str(input_shape) + '`')\n",
        "                if ((input_shape[0] is not None and input_shape[0] < min_size) or\n",
        "                   (input_shape[1] is not None and input_shape[1] < min_size)):\n",
        "                    raise ValueError('Input size must be at least ' +\n",
        "                                     str(min_size) + 'x' + str(min_size) +\n",
        "                                     '; got `input_shape=' +\n",
        "                                     str(input_shape) + '`')\n",
        "    else:\n",
        "        if require_flatten:\n",
        "            input_shape = default_shape\n",
        "        else:\n",
        "            if data_format == 'channels_first':\n",
        "                input_shape = (3, None, None)\n",
        "            else:\n",
        "                input_shape = (None, None, 3)\n",
        "    if require_flatten:\n",
        "        if None in input_shape:\n",
        "            raise ValueError('If `include_top` is True, '\n",
        "                             'you should specify a static `input_shape`. '\n",
        "                             'Got `input_shape=' + str(input_shape) + '`')\n",
        "    return input_shape\n",
        "\n",
        "def ResNet(input_shape=None,\n",
        "           initial_conv_filters=64,\n",
        "           depth=[3, 4, 6, 3],\n",
        "           filters=[64, 128, 256, 512],\n",
        "           width=1,\n",
        "           bottleneck=False,\n",
        "           weight_decay=1e-4,\n",
        "           include_top=True,\n",
        "           weights=None,\n",
        "           input_tensor=None,\n",
        "           pooling=None,\n",
        "           classes=1000):\n",
        "    \"\"\" Instantiate the ResNet architecture. Note that ,\n",
        "        when using TensorFlow for best performance you should set\n",
        "        `image_data_format=\"channels_last\"` in your Keras config\n",
        "        at ~/.keras/keras.json.\n",
        "        The model are compatible with both\n",
        "        TensorFlow and Theano. The dimension ordering\n",
        "        convention used by the model is the one\n",
        "        specified in your Keras config file.\n",
        "        # Arguments\n",
        "            initial_conv_filters: number of features for the initial convolution\n",
        "            depth: number or layers in the each block, defined as a list.\n",
        "                ResNet-50  = [3, 4, 6, 3]\n",
        "                ResNet-101 = [3, 6, 23, 3]\n",
        "                ResNet-152 = [3, 8, 36, 3]\n",
        "            filter: number of filters per block, defined as a list.\n",
        "                filters = [64, 128, 256, 512\n",
        "            width: width multiplier for the network (for Wide ResNets)\n",
        "            bottleneck: adds a bottleneck conv to reduce computation\n",
        "            weight_decay: weight decay (l2 norm)\n",
        "            include_top: whether to include the fully-connected\n",
        "                layer at the top of the network.\n",
        "            weights: `None` (random initialization) or `imagenet` (trained\n",
        "                on ImageNet)\n",
        "            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
        "                to use as image input for the model.\n",
        "            input_shape: optional shape tuple, only to be specified\n",
        "                if `include_top` is False (otherwise the input shape\n",
        "                has to be `(224, 224, 3)` (with `tf` dim ordering)\n",
        "                or `(3, 224, 224)` (with `th` dim ordering).\n",
        "                It should have exactly 3 inputs channels,\n",
        "                and width and height should be no smaller than 8.\n",
        "                E.g. `(200, 200, 3)` would be one valid value.\n",
        "            pooling: Optional pooling mode for feature extraction\n",
        "                when `include_top` is `False`.\n",
        "                - `None` means that the output of the model will be\n",
        "                    the 4D tensor output of the\n",
        "                    last convolutional layer.\n",
        "                - `avg` means that global average pooling\n",
        "                    will be applied to the output of the\n",
        "                    last convolutional layer, and thus\n",
        "                    the output of the model will be a 2D tensor.\n",
        "                - `max` means that global max pooling will\n",
        "                    be applied.\n",
        "            classes: optional number of classes to classify images\n",
        "                into, only to be specified if `include_top` is True, and\n",
        "                if no `weights` argument is specified.\n",
        "        # Returns\n",
        "            A Keras model instance.\n",
        "        \"\"\"\n",
        "\n",
        "    if weights not in {'imagenet', None}:\n",
        "        raise ValueError('The `weights` argument should be either '\n",
        "                         '`None` (random initialization) or `imagenet` '\n",
        "                         '(pre-training on ImageNet).')\n",
        "\n",
        "    if weights == 'imagenet' and include_top and classes != 1000:\n",
        "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
        "                         ' as true, `classes` should be 1000')\n",
        "\n",
        "    assert len(depth) == len(filters), \"The length of filter increment list must match the length \" \\\n",
        "                                       \"of the depth list.\"\n",
        "\n",
        "    # Determine proper input shape\n",
        "    input_shape = _obtain_input_shape(input_shape,\n",
        "                                      default_size=224,\n",
        "                                      min_size=32,\n",
        "                                      data_format=K.image_data_format(),\n",
        "                                      require_flatten=False)\n",
        "\n",
        "    if input_tensor is None:\n",
        "        img_input = Input(shape=input_shape)\n",
        "    else:\n",
        "        if not K.is_keras_tensor(input_tensor):\n",
        "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
        "        else:\n",
        "            img_input = input_tensor\n",
        "\n",
        "    x = _create_resnet(classes, img_input, include_top, initial_conv_filters,\n",
        "                       filters, depth, width, bottleneck, weight_decay, pooling)\n",
        "\n",
        "    # Ensure that the model takes into account\n",
        "    # any potential predecessors of `input_tensor`.\n",
        "    if input_tensor is not None:\n",
        "        inputs = get_source_inputs(input_tensor)\n",
        "    else:\n",
        "        inputs = img_input\n",
        "    # Create model.\n",
        "    model = Model(inputs, x, name='resnext')\n",
        "\n",
        "    # load weights\n",
        "\n",
        "    return model\n",
        "\n",
        "def ResNet18(input_shape=None,\n",
        "             width=1,\n",
        "             bottleneck=False,\n",
        "             weight_decay=1e-4,\n",
        "             include_top=True,\n",
        "             weights=None,\n",
        "             input_tensor=None,\n",
        "             pooling=None,\n",
        "             classes=1000):\n",
        "    return ResNet(input_shape,\n",
        "                  depth=[2, 2, 2, 2],\n",
        "                  width=width,\n",
        "                  bottleneck=bottleneck,\n",
        "                  weight_decay=weight_decay,\n",
        "                  include_top=include_top,\n",
        "                  weights=weights,\n",
        "                  input_tensor=input_tensor,\n",
        "                  pooling=pooling,\n",
        "                  classes=classes)\n",
        "\n",
        "\n",
        "def ResNet34(input_shape=None,\n",
        "             width=1,\n",
        "             bottleneck=False,\n",
        "             weight_decay=1e-4,\n",
        "             include_top=True,\n",
        "             weights=None,\n",
        "             input_tensor=None,\n",
        "             pooling=None,\n",
        "             classes=1000):\n",
        "    return ResNet(input_shape,\n",
        "                  depth=[3, 4, 6, 3],\n",
        "                  width=width,\n",
        "                  bottleneck=bottleneck,\n",
        "                  weight_decay=weight_decay,\n",
        "                  include_top=include_top,\n",
        "                  weights=weights,\n",
        "                  input_tensor=input_tensor,\n",
        "                  pooling=pooling,\n",
        "                  classes=classes)\n",
        "\n",
        "\n",
        "def ResNet50(input_shape=None,\n",
        "             width=1,\n",
        "             bottleneck=True,\n",
        "             weight_decay=1e-4,\n",
        "             include_top=True,\n",
        "             weights=None,\n",
        "             input_tensor=None,\n",
        "             pooling=None,\n",
        "             classes=1000):\n",
        "    return ResNet(input_shape,\n",
        "                  width=width,\n",
        "                  bottleneck=bottleneck,\n",
        "                  weight_decay=weight_decay,\n",
        "                  include_top=include_top,\n",
        "                  weights=weights,\n",
        "                  input_tensor=input_tensor,\n",
        "                  pooling=pooling,\n",
        "                  classes=classes)\n",
        "\n",
        "\n",
        "def ResNet101(input_shape=None,\n",
        "              width=1,\n",
        "              bottleneck=True,\n",
        "              weight_decay=1e-4,\n",
        "              include_top=True,\n",
        "              weights=None,\n",
        "              input_tensor=None,\n",
        "              pooling=None,\n",
        "              classes=1000):\n",
        "    return ResNet(input_shape,\n",
        "                  depth=[3, 6, 23, 3],\n",
        "                  width=width,\n",
        "                  bottleneck=bottleneck,\n",
        "                  weight_decay=weight_decay,\n",
        "                  include_top=include_top,\n",
        "                  weights=weights,\n",
        "                  input_tensor=input_tensor,\n",
        "                  pooling=pooling,\n",
        "                  classes=classes)\n",
        "\n",
        "\n",
        "def ResNet154(input_shape=None,\n",
        "              width=1,\n",
        "              bottleneck=True,\n",
        "              weight_decay=1e-4,\n",
        "              include_top=True,\n",
        "              weights=None,\n",
        "              input_tensor=None,\n",
        "              pooling=None,\n",
        "              classes=1000):\n",
        "    return ResNet(input_shape,\n",
        "                  depth=[3, 8, 36, 3],\n",
        "                  width=width,\n",
        "                  bottleneck=bottleneck,\n",
        "                  weight_decay=weight_decay,\n",
        "                  include_top=include_top,\n",
        "                  weights=weights,\n",
        "                  input_tensor=input_tensor,\n",
        "                  pooling=pooling,\n",
        "                  classes=classes)\n",
        "\n",
        "\n",
        "def _resnet_block(input, filters, k=1, strides=(1, 1)):\n",
        "    ''' Adds a pre-activation resnet block without bottleneck layers\n",
        "    Args:\n",
        "        input: input tensor\n",
        "        filters: number of output filters\n",
        "        k: width factor\n",
        "        strides: strides of the convolution layer\n",
        "    Returns: a keras tensor\n",
        "    '''\n",
        "    init = input\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "\n",
        "    x = GroupNormalization(axis=channel_axis)(input)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if strides != (1, 1) or init._keras_shape[channel_axis] != filters * k:\n",
        "        init = Conv2D(filters * k, (1, 1), padding='same', kernel_initializer='he_normal',\n",
        "                      use_bias=False, strides=strides)(x)\n",
        "\n",
        "    x = Conv2D(filters * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "               use_bias=False, strides=strides)(x)\n",
        "    x = GroupNormalization(axis=channel_axis, gamma_initializer='zeros')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "               use_bias=False)(x)\n",
        "\n",
        "    m = add([x, init])\n",
        "    return m\n",
        "\n",
        "\n",
        "def _resnet_bottleneck_block(input, filters, k=1, strides=(1, 1)):\n",
        "    ''' Adds a pre-activation resnet block with bottleneck layers\n",
        "    Args:\n",
        "        input: input tensor\n",
        "        filters: number of output filters\n",
        "        k: width factor\n",
        "        strides: strides of the convolution layer\n",
        "    Returns: a keras tensor\n",
        "    '''\n",
        "    init = input\n",
        "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
        "    bottleneck_expand = 4\n",
        "\n",
        "    x = GroupNormalization(axis=channel_axis)(input)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if strides != (1, 1) or init._keras_shape[channel_axis] != bottleneck_expand * filters * k:\n",
        "        init = Conv2D(bottleneck_expand * filters * k, (1, 1), padding='same', kernel_initializer='he_normal',\n",
        "                      use_bias=False, strides=strides)(x)\n",
        "\n",
        "    x = Conv2D(filters * k, (1, 1), padding='same', kernel_initializer='he_normal',\n",
        "               use_bias=False)(x)\n",
        "    x = GroupNormalization(axis=channel_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(filters * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
        "               use_bias=False, strides=strides)(x)\n",
        "    x = GroupNormalization(axis=channel_axis, gamma_initializer='zeros')(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    x = Conv2D(bottleneck_expand * filters * k, (1, 1), padding='same', kernel_initializer='he_normal',\n",
        "               use_bias=False)(x)\n",
        "\n",
        "    m = add([x, init])\n",
        "    return m\n",
        "\n",
        "\n",
        "def _create_resnet(classes, img_input, include_top, initial_conv_filters, filters,\n",
        "                   depth, width, bottleneck, weight_decay, pooling):\n",
        "    '''Creates a SE ResNet model with specified parameters\n",
        "    Args:\n",
        "        initial_conv_filters: number of features for the initial convolution\n",
        "        include_top: Flag to include the last dense layer\n",
        "        filters: number of filters per block, defined as a list.\n",
        "            filters = [64, 128, 256, 512\n",
        "        depth: number or layers in the each block, defined as a list.\n",
        "            ResNet-50  = [3, 4, 6, 3]\n",
        "            ResNet-101 = [3, 6, 23, 3]\n",
        "            ResNet-152 = [3, 8, 36, 3]\n",
        "        width: width multiplier for network (for Wide ResNet)\n",
        "        bottleneck: adds a bottleneck conv to reduce computation\n",
        "        weight_decay: weight_decay (l2 norm)\n",
        "        pooling: Optional pooling mode for feature extraction\n",
        "            when `include_top` is `False`.\n",
        "            - `None` means that the output of the model will be\n",
        "                the 4D tensor output of the\n",
        "                last convolutional layer.\n",
        "            - `avg` means that global average pooling\n",
        "                will be applied to the output of the\n",
        "                last convolutional layer, and thus\n",
        "                the output of the model will be a 2D tensor.\n",
        "            - `max` means that global max pooling will\n",
        "                be applied.\n",
        "    Returns: a Keras Model\n",
        "    '''\n",
        "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "    N = list(depth)\n",
        "\n",
        "    # block 1 (initial conv block)\n",
        "    x = Conv2D(initial_conv_filters, (7, 7), padding='same', use_bias=False, strides=(2, 2),\n",
        "               kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(img_input)\n",
        "\n",
        "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
        "\n",
        "    # block 2 (projection block)\n",
        "    for i in range(N[0]):\n",
        "        if bottleneck:\n",
        "            x = _resnet_bottleneck_block(x, filters[0], width)\n",
        "        else:\n",
        "            x = _resnet_block(x, filters[0], width)\n",
        "\n",
        "    # block 3 - N\n",
        "    for k in range(1, len(N)):\n",
        "        if bottleneck:\n",
        "            x = _resnet_bottleneck_block(x, filters[k], width, strides=(2, 2))\n",
        "        else:\n",
        "            x = _resnet_block(x, filters[k], width, strides=(2, 2))\n",
        "\n",
        "        for i in range(N[k] - 1):\n",
        "            if bottleneck:\n",
        "                x = _resnet_bottleneck_block(x, filters[k], width)\n",
        "            else:\n",
        "                x = _resnet_block(x, filters[k], width)\n",
        "\n",
        "    x = GroupNormalization(axis=channel_axis)(x)\n",
        "    x = Activation('relu')(x)\n",
        "\n",
        "    if include_top:\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "        x = Dense(classes, use_bias=False, kernel_regularizer=l2(weight_decay),\n",
        "                  activation='softmax')(x)\n",
        "    else:\n",
        "        if pooling == 'avg':\n",
        "            x = GlobalAveragePooling2D()(x)\n",
        "        elif pooling == 'max':\n",
        "            x = GlobalMaxPooling2D()(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "33dca9ccb84c29498162514c74eec2c1ccb46557",
        "id": "UStDoFhf636B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import *\n",
        "from keras.applications.resnet50 import decode_predictions\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import metrics\n",
        "from keras.optimizers import Adam \n",
        "from keras.regularizers import *\n",
        "from keras import backend as K\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "\n",
        "from keras.applications import (\n",
        "    DenseNet121,\n",
        "    DenseNet169,\n",
        "    DenseNet201,\n",
        "    MobileNet,\n",
        "    InceptionResNetV2,\n",
        "    InceptionV3,\n",
        "    ResNet50,\n",
        "    Xception,\n",
        "    NASNetMobile,\n",
        "    NASNetLarge,\n",
        "    VGG16\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0oLzDj2_CLog",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def rgb_to_grayscale(input):\n",
        "    \"\"\"Average out each pixel across its 3 RGB layers resulting in a grayscale image\"\"\"\n",
        "    return K.mean(input, axis=3)\n",
        "\n",
        "\n",
        "def rgb_to_grayscale_output_shape(input_shape):\n",
        "    return input_shape[:-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W-Z98jrjCVby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.constraints import max_norm\n",
        "\n",
        "def create_model(input_shape, n_out):\n",
        "    ResNet50model = ResNet50(include_top=False,\n",
        "                                   weights=\"imagenet\",\n",
        "                                   input_tensor=None,\n",
        "                                   input_shape=input_shape,\n",
        "                                   pooling=None)\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "    gn = GroupNormalization(groups=3)(input_tensor)\n",
        "    x = ResNet50model(gn)\n",
        "    #x = DenseNet121model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu',kernel_constraint=max_norm(2.))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    cnn_bottleneck = Dense(1024, activation='relu')(x)\n",
        "    \n",
        "    x = Lambda(rgb_to_grayscale, rgb_to_grayscale_output_shape)(input_tensor)\n",
        "    x = Reshape((23, 3887))(x) \n",
        "    lstm = keras.layers.Bidirectional(keras.layers.CuDNNGRU(units=16,\n",
        "                                                    return_sequences=True))(x)\n",
        "    att = SeqSelfAttention(attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
        "                       kernel_regularizer=keras.regularizers.l2(1e-4),\n",
        "                       bias_regularizer=keras.regularizers.l1(1e-4),\n",
        "                       attention_regularizer_weight=1e-4,\n",
        "                       name='Attention')(lstm)\n",
        "\n",
        "    flatten = keras.layers.Flatten()(att)\n",
        "    dense = Dense(1024, activation=\"relu\",kernel_constraint=max_norm(2.))(flatten)\n",
        "    dropout = Dropout(0.5)(dense)\n",
        "    \n",
        "    \n",
        "    x = concatenate([cnn_bottleneck,dropout])\n",
        "    predictions = Dense(n_out, activation='sigmoid')(x)\n",
        "    \n",
        "    model = Model(input=input_tensor, output=predictions)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "40bc05ad3927bd074aa1f46bdc2a0840bf56fd51",
        "id": "d6f1-Z4q636H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Learning Rate Finder"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "67fdab5d4f2c1c035f6fae736f4635d722fb5bf5",
        "id": "PJOGVhFM636I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "from keras.callbacks import Callback\n",
        "\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    \n",
        "    '''\n",
        "    A simple callback for finding the optimal learning rate range for your model + dataset. \n",
        "    \n",
        "    # Usage\n",
        "        ```python\n",
        "            lr_finder = LRFinder(min_lr=1e-5, \n",
        "                                 max_lr=1e-2, \n",
        "                                 steps_per_epoch=np.ceil(epoch_size/batch_size), \n",
        "                                 epochs=3)\n",
        "            model.fit(X_train, Y_train, callbacks=[lr_finder])\n",
        "            \n",
        "            lr_finder.plot_loss()\n",
        "        ```\n",
        "    \n",
        "    # Arguments\n",
        "        min_lr: The lower bound of the learning rate range for the experiment.\n",
        "        max_lr: The upper bound of the learning rate range for the experiment.\n",
        "        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size/batch_size)`. \n",
        "        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n",
        "        \n",
        "    # References\n",
        "        Blog post: jeremyjordan.me/nn-learning-rate\n",
        "        Original paper: https://arxiv.org/abs/1506.01186\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.total_iterations = steps_per_epoch * epochs\n",
        "        self.iteration = 0\n",
        "        self.history = {}\n",
        "        \n",
        "    def clr(self):\n",
        "        '''Calculate the learning rate.'''\n",
        "        x = self.iteration / self.total_iterations \n",
        "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        '''Initialize the learning rate to the minimum value at the start of training.'''\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.min_lr)\n",
        "        \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        '''Record previous batch statistics and update the learning rate.'''\n",
        "        logs = logs or {}\n",
        "        self.iteration += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.iteration)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "            \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        " \n",
        "    def plot_lr(self):\n",
        "        '''Helper function to quickly inspect the learning rate schedule.'''\n",
        "        plt.plot(self.history['iterations'], self.history['lr'])\n",
        "        plt.yscale('log')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Learning rate')\n",
        "        plt.show()\n",
        "        \n",
        "    def plot_loss(self):\n",
        "        '''Helper function to quickly observe the learning rate experiment results.'''\n",
        "        plt.plot(self.history['lr'], self.history['loss'])\n",
        "        plt.xscale('log')\n",
        "        plt.xlabel('Learning rate')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6b934ce654fb9e0e8263b11ec21f1312185a61a2",
        "id": "dqhFgwrp636K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create callbacks list\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "epochs = 15; batch_size = 16\n",
        "checkpoint = ModelCheckpoint('rn50_lstm.h5', monitor='val_loss', verbose=1, \n",
        "                             save_best_only=True, mode='min', save_weights_only = True)\n",
        "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n",
        "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
        "early = EarlyStopping(monitor=\"val_loss\", \n",
        "                      mode=\"min\", \n",
        "                      patience=6)\n",
        "\n",
        "lr_finder = LRFinder(min_lr=1e-6,\n",
        "                     max_lr=1e-2, \n",
        "                     steps_per_epoch=np.ceil(epochs/batch_size), \n",
        "                     epochs=3)\n",
        "\n",
        "callbacks_list = [checkpoint, reduceLROnPlat, early]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07d8b83fed9e5ac0faa3c4c8f9e3731916e291ba",
        "id": "7YU_XPvg636M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0cd5ca2b-6b85-4c8d-ea88-6df13c15ab74"
      },
      "cell_type": "code",
      "source": [
        "data['target_list'] = data['Target'].map(lambda x: [int(a) for a in x.split(' ')])\n",
        "all_labels = list(chain.from_iterable(data['target_list'].values))\n",
        "c_val = Counter(all_labels)\n",
        "n_keys = c_val.keys()\n",
        "max_idx = max(n_keys)\n",
        "data['target_vec'] = data['target_list'].map(lambda ck: [i in ck for i in range(max_idx+1)])\n",
        "from sklearn.model_selection import train_test_split\n",
        "lab_vec = data['target_list'].map(lambda ck: [int(i in ck) for i in range(28)])\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "\n",
        "spl = MultilabelStratifiedShuffleSplit(n_splits=1,\n",
        "                 test_size = 0.2, random_state=42)\n",
        "for ti, vi in spl.split(np.zeros(len(data)), lab_vec.tolist()):\n",
        "        train_df = data.ix[ti]\n",
        "        valid_df = data.ix[vi]\n",
        "print(train_df.shape[0], 'training masks')\n",
        "print(valid_df.shape[0], 'validation masks')\n",
        "train_df.to_csv('train_part.csv')\n",
        "valid_df.to_csv('valid_part.csv')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24837 training masks\n",
            "6235 validation masks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e84e27b19109377232656479335d102baa7e65d6",
        "id": "pEW0TdQi636P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create train and valid datagens\n",
        "train_generator = data_generator.create_train(\n",
        "    train_dataset_info, batch_size, (SIZE,SIZE,3), augument=True)\n",
        "validation_generator = data_generator.create_train(\n",
        "    train_dataset_info, 32, (SIZE,SIZE,3), augument=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "52137b79097e7ed9220a2fdae5928b751b48b394",
        "id": "s_WlKAli636T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create train datagen\n",
        "train_datagen = data_generator()\n",
        "\n",
        "generator = train_datagen.create_train(\n",
        "    train_dataset_info, 5, (299,299,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "69d4ca0fb062b3ccdb144d69fb0987d57f64e869",
        "id": "_Q6gJNW4636W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data['target_list'] = data['Target'].map(lambda x: [int(a) for a in x.split(' ')])\n",
        "all_labels = list(chain.from_iterable(data['target_list'].values))\n",
        "c_val = Counter(all_labels)\n",
        "n_keys = c_val.keys()\n",
        "max_idx = max(n_keys)\n",
        "data['target_vec'] = data['target_list'].map(lambda ck: [i in ck for i in range(max_idx+1)])\n",
        "lab_vec = data['target_list'].map(lambda ck: [int(i in ck) for i in range(28)])\n",
        "\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
        "spl = MultilabelStratifiedShuffleSplit(\n",
        "   n_splits=1, test_size=0.1, random_state=42)\n",
        "train_indexes, valid_indexes =  next(spl.split(np.zeros(len(data)), lab_vec.tolist()))\n",
        "np.save('rn50_LSTM_train_indexes.npy', train_indexes)\n",
        "np.save('rn50_LSTM_valid_indexes.npy', valid_indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43456de9cd1dd74427495ae6966d216dbf501b74",
        "id": "VxvtvfEO636Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "7240b87c-8a07-4a43-a7fc-144d77304683"
      },
      "cell_type": "code",
      "source": [
        "# warm up model\n",
        "model = create_model(\n",
        "    input_shape=(SIZE,SIZE,3), \n",
        "    n_out=28)\n",
        "\n",
        "\n",
        "for layer in model.layers[:-6]:\n",
        "    layer.trainable = False\n",
        "    \n",
        "for layer in model.layers:\n",
        "    print(layer, layer.trainable)\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy', \n",
        "    optimizer=Adam(1e-03),\n",
        "    metrics=['acc'])\n",
        "\n",
        "#model.load_weights('../input/fork-of-resnet50-groupnorm-baseline/Resnet50.h5')\n",
        "\n",
        "# model.summary()\n",
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
        "    epochs=2,\n",
        "    shuffle=True,\n",
        "    verbose=1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.engine.input_layer.InputLayer object at 0x7f1c9e0daf60> False\n",
            "<keras.layers.core.Lambda object at 0x7f1c9aed6198> False\n",
            "<__main__.GroupNormalization object at 0x7f1c9e0610b8> False\n",
            "<keras.layers.core.Reshape object at 0x7f1c9aef2f28> False\n",
            "<keras.engine.training.Model object at 0x7f1c9e0daa20> False\n",
            "<keras.layers.wrappers.Bidirectional object at 0x7f1c9aeb7a90> False\n",
            "<keras.layers.pooling.GlobalAveragePooling2D object at 0x7f1c9d6d5cc0> False\n",
            "<keras_self_attention.seq_self_attention.SeqSelfAttention object at 0x7f1c9ae57470> False\n",
            "<keras.layers.core.Dense object at 0x7f1c9d73ffd0> False\n",
            "<keras.layers.core.Flatten object at 0x7f1c946a4fd0> False\n",
            "<keras.layers.core.Dropout object at 0x7f1c9af006d8> True\n",
            "<keras.layers.core.Dense object at 0x7f1c946c6080> True\n",
            "<keras.layers.core.Dense object at 0x7f1c9af86a20> True\n",
            "<keras.layers.core.Dropout object at 0x7f1c945ed748> True\n",
            "<keras.layers.merge.Concatenate object at 0x7f1c945ed908> True\n",
            "<keras.layers.core.Dense object at 0x7f1c945ed860> True\n",
            "Epoch 1/2\n",
            "1748/1748 [==============================] - 817s 467ms/step - loss: 0.1634 - acc: 0.9474 - val_loss: 0.1887 - val_acc: 0.9364\n",
            "Epoch 2/2\n",
            "1748/1748 [==============================] - 792s 453ms/step - loss: 0.1557 - acc: 0.9490 - val_loss: 0.1867 - val_acc: 0.9419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1c9454ecf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d21ca1dee15e3fa389b3c1ac4c30c44358a3ed2",
        "id": "cbUYm4Cp636e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "892496a4-a158-42f8-e826-f73779c514df"
      },
      "cell_type": "code",
      "source": [
        "# train all layers\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "model.compile(loss='binary_crossentropy',\n",
        "            optimizer=Adam(lr=1e-4),\n",
        "            metrics=['accuracy'])\n",
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=np.ceil(float(len(train_indexes)) / float(batch_size)),\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=np.ceil(float(len(valid_indexes)) / float(batch_size)),\n",
        "    epochs=epochs, \n",
        "    verbose=1,\n",
        "    shuffle=True,\n",
        "    callbacks=callbacks_list)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "1748/1748 [==============================] - 1744s 997ms/step - loss: 0.1332 - acc: 0.9556 - val_loss: 0.1259 - val_acc: 0.9565\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.12594, saving model to rn50_lstm.h5\n",
            "Epoch 2/15\n",
            "1748/1748 [==============================] - 1709s 978ms/step - loss: 0.1121 - acc: 0.9615 - val_loss: 0.1024 - val_acc: 0.9645\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.12594 to 0.10241, saving model to rn50_lstm.h5\n",
            "Epoch 3/15\n",
            " 141/1748 [=>............................] - ETA: 24:31 - loss: 0.1073 - acc: 0.9631"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2a59b0157be9b6bb724b549c5025ff0859cdb242",
        "id": "I9Vpt97w636h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create submit\n",
        "submit = pd.read_csv('sample_submission.csv')\n",
        "predicted = []\n",
        "draw_predict = []\n",
        "model.load_weights('rn50_lstm.h5',) #custom_objects=SeqSelfAttention.get_custom_objects()\n",
        "for name in tqdm(submit['Id']):\n",
        "    path = os.path.join('test/', name)\n",
        "    image = data_generator.load_image(path, (SIZE,SIZE,3))/255.\n",
        "    score_predict = model.predict(image[np.newaxis])[0]\n",
        "    draw_predict.append(score_predict)\n",
        "    label_predict = np.arange(28)[score_predict>=0.2]\n",
        "    str_predict_label = ' '.join(str(l) for l in label_predict)\n",
        "    predicted.append(str_predict_label)\n",
        "\n",
        "\n",
        "submit['Predicted'] = predicted\n",
        "np.save('draw_predict_rn50_lstm.npy', score_predict)\n",
        "submit.to_csv('submit_rn50_lstm.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YtA4Z65177-y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions submit human-protein-atlas-image-classification -f submit_rn50_lstm.csv -m \"My submission message\""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}